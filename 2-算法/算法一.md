why:机器学习的核心

what:
监督：有标签值
无监督：有标签值，标签值需要被求出

## 线性回归
回归，无穷时趋向于一个值
分类：

## 决策树
<font color='red'>**熵**</font>	  越大，纯度越低
<font color='red'>**Gini系数**</font>   越大，纯度越低
在决策树的优化时，概率*纯度，越低的，越先决策

<font color='red'>**信息增溢**</font>=原始熵值-此属性下的信息熵，信息增溢值越大，越先决策
目的：信息增溢越大，使信息熵值下降越迅速，即最优先决策属性；

<font color='red'>**信息增溢率**</font>=信息增溢/熵

目标函数

<font color='red'>**评价函数**</font>：叶子节点的熵值越小越好，叶子节点个数=权重

连续型的属性，可离散化处理

<font color='red'>**剪枝**</font>
预剪枝：在构建决策树的过程中，提前停止
后剪枝：决策树构建好后，然后才开始剪枝
叶子节点个数越多，损失越大

<font color='red'>**随机森林**</font>
在有放回采样的基础上
随机：样本随机、特征随机
森林：多颗决策树，最终结果由多颗树决定出来（随机样本）

<font color='red'>**决策树参数**</font>






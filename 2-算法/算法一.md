why:机器学习的核心

what:
监督：有标签值
无监督：有标签值，标签值需要被求出

how:
## 线性回归
<font color='red'>**线性回归**</font>
回归，无穷时趋向于一个值
分类：
样本值、真实值：真实值=观测值+误差，误差一般符合高斯分布
似然函数：求sita为多少时，和X组合后越拉近真实值Y的函数，概率P最大

<font color='red'>**逻辑回归**</font>
Sigmoid函数，值域[0,1]，把回归算法转化为分类问题

<font color='red'>**梯度下降**</font>
求偏导，求极值点
学习率
一般不通过线性回归或逻辑回归求最优解，因为


## 决策树
<font color='red'>**熵**</font>	  越大，纯度越低
<font color='red'>**Gini系数**</font>   越大，纯度越低
在决策树的优化时，概率*纯度，越低的，越先决策

<font color='red'>**信息增溢**</font>=原始熵值-此属性下的信息熵，信息增溢值越大，越先决策
目的：信息增溢越大，使信息熵值下降越迅速，即最优先决策属性；

<font color='red'>**信息增溢率**</font>=信息增溢/熵

目标函数

<font color='red'>**评价函数**</font>：叶子节点的熵值越小越好，叶子节点个数=权重

连续型的属性，可离散化处理

<font color='red'>**剪枝**</font>
预剪枝：在构建决策树的过程中，提前停止
后剪枝：决策树构建好后，然后才开始剪枝
叶子节点个数越多，损失越大

<font color='red'>**随机森林**</font>
在有放回采样的基础上
随机：样本随机、特征随机
森林：多颗决策树，最终结果由多颗树决定出来（随机样本）

<font color='red'>**决策树参数**</font>





